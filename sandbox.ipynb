{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *Initial* **Setup**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Package** *Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\sousa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: backcall in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.3.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\sousa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pydicom\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports (General)\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Library Imports (Modelling)\n",
    "import pydicom\n",
    "import torch\n",
    "import scipy\n",
    "\n",
    "# Library Imports (Monitoring)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Imports (General)\n",
    "from pathlib import Path\n",
    "from ipywidgets import interactive, IntSlider\n",
    "\n",
    "# Function Imports (Modelling)\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Function Imports (Monitoring)\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Argument** *Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Conditional 3D Diffusion Model Parser Initialization\n",
    "ncdiff_parser = argparse.ArgumentParser(\n",
    "    description = \"Non-Conditional 3D Diffusion Model\")\n",
    "ncdiff_parser.add_argument('--model_type', type = str,            # Chosen Model / Diffusion\n",
    "                            choices =  {'video_diffusion',\n",
    "                                        'blackout_diffusion',\n",
    "                                        'gamma_diffusion'},\n",
    "                            default = 'video_diffusion')\n",
    "ncdiff_parser.add_argument('--model_version', type = int,         # Model Version Index\n",
    "                            default = 0)\n",
    "ncdiff_parser.add_argument('--data_version', type = int,          # Dataset Version Index\n",
    "                            default = 0)\n",
    "settings = ncdiff_parser.parse_args(\"\")\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Directories and Path Arguments\n",
    "ncdiff_parser.add_argument('--reader_folderpath', type = str,         # Path for Dataset Reader Directory\n",
    "                            default = 'data/non_cond')\n",
    "ncdiff_parser.add_argument('--public_data_folderpath', type = str,    # Path for Private Dataset Directory\n",
    "                            default = \"X:/nas-ctm01/datasets/public/MEDICAL/Duke-Breast-Cancer-T1\")\n",
    "                            #default = \"../../datasets/public/MEDICAL/Duke-Breast-Cancer-T1\")\n",
    "ncdiff_parser.add_argument('--private_data_folderpath', type = str,   # Path for Private Dataset Directory\n",
    "                            default = \"X:/nas-ctm01/datasets/private/METABREST/T1W_Breast\")\n",
    "                            #default = '../../datasets/private/METABREST/T1W_Breast')\n",
    "\n",
    "# Directory | Model-Related Path Arguments\n",
    "ncdiff_parser.add_argument('--model_folderpath', type = str,          # Path for Model Architecture Directory\n",
    "                            default = f'models/{settings.model_type}')\n",
    "ncdiff_parser.add_argument('--script_folderpath', type = str,         # Path for Model Training & Testing Scripts Directory\n",
    "                            default = f'scripts/{settings.model_type}')\n",
    "ncdiff_parser.add_argument('--logs_folderpath', type = str,           # Path for Model Saving Directory\n",
    "                            default = f'logs/{settings.model_type}')\n",
    "    \n",
    "# ============================================================================================\n",
    "\n",
    "# Dataset | Dataset General Arguments\n",
    "ncdiff_parser.add_argument('--img_size', type = int,              # Generated Image Resolution\n",
    "                            default = 64)\n",
    "ncdiff_parser.add_argument('--num_slice', type = int,             # Number of 2D Slices in MRI\n",
    "                            default = 30)\n",
    "ncdiff_parser.add_argument('--data_prep', type = bool,            # Usage of Dataset Pre-Processing Control Value\n",
    "                            default = True)\n",
    "ncdiff_parser.add_argument('--h_flip', type = int,                # Percentage of Horizontally Flipped Subjects\n",
    "                            default = 50)\n",
    "\n",
    "# Dataset | Dataset Splitting Arguments\n",
    "ncdiff_parser.add_argument('--train_subj', type = int,            # Number of Random Subjects in Training Set\n",
    "                            default = 20)                         # PS: Input 0 for all Subjects in the Dataset\n",
    "ncdiff_parser.add_argument('--val_subj', type = int,              # Number of Random Subjects in Validation Set\n",
    "                            default = 0)\n",
    "ncdiff_parser.add_argument('--test_subj', type = int,             # Number of Random Subjects in Test Set\n",
    "                            default = 0)\n",
    "\n",
    "# Dataset | DataLoader Arguments\n",
    "ncdiff_parser.add_argument('--batch_size', type = int,            # DataLoader Batch Size Value\n",
    "                            default = 1)\n",
    "ncdiff_parser.add_argument('--shuffle', type = bool,              # DataLoader Subject Shuffling Control Value\n",
    "                            default = False)\n",
    "ncdiff_parser.add_argument('--num_workers', type = int,           # Number of DataLoader Workers\n",
    "                            default = 12)\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Model | Architecture-Defining Arguments\n",
    "ncdiff_parser.add_argument('--seed', type = int,                  # Randomised Generational Seed\n",
    "                            default = 0)\n",
    "ncdiff_parser.add_argument('--dim', type = int,                   # Input Dimensionality (Not Necessary)\n",
    "                            default = 64)\n",
    "ncdiff_parser.add_argument('--num_channel', type = int,           # Number of Input Channels for Dataset\n",
    "                            default = 1)\n",
    "ncdiff_parser.add_argument('--mult_dim', type = tuple,            # Dimensionality for all Conditional Layers\n",
    "                            default = (1, 2, 4, 8))\n",
    "\n",
    "# Model | Training & Diffusion Arguments\n",
    "#ncdiff_parser.add_argument('--num_epochs', type = int,            # Number of Training Epochs\n",
    "#                            default = 30)\n",
    "ncdiff_parser.add_argument('--num_ts', type = int,                # Number of Scheduler Timesteps\n",
    "                            default = 300)\n",
    "ncdiff_parser.add_argument('--num_steps', type = int,             # Number of Diffusion Training Steps\n",
    "                            default = 10000)\n",
    "ncdiff_parser.add_argument('--lr_base', type = float,             # Base Learning Rate Value\n",
    "                            default = 1e-3)\n",
    "ncdiff_parser.add_argument('--save_interval', type = int,         # Number of Training Step Interval inbetween Image Saving\n",
    "                            default = 1000)\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "settings = ncdiff_parser.parse_args(\"\")\n",
    "settings.device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *Dataset* **Access**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Raw** *Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Station\n",
    "patient_id = 26\n",
    "patient_pos = 'OFP'\n",
    "num_slice = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'OFP/ID26'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset Access\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m slice_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpatient_pos\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ID\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpatient_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m data_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ID\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslice_list[num_slice]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m pydicom\u001b[38;5;241m.\u001b[39mdcmread(data_filepath)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'OFP/ID26'"
     ]
    }
   ],
   "source": [
    "# Dataset Access\n",
    "slice_list = os.listdir(f'{patient_pos}/ID{patient_id}')\n",
    "data_filepath = f\"{patient_pos}/ID{patient_id}/{slice_list[num_slice]}\"\n",
    "raw_data = pydicom.dcmread(data_filepath)\n",
    "pixel_data = raw_data.pixel_array\n",
    "\n",
    "# Meta Data Initialization\n",
    "#meta_filepath = \"ID1/S201-I-_PRIM_M_SE_T1W_TSE-20170324/annotFile_bkp.mat\"\n",
    "#meta_data = scipy.io.loadmat(meta_filepath)\n",
    "#pixel_data = np.frombuffer(raw_data.PixelData)\n",
    "#pixel_data = pydicom.data.get_testdata_file(data_filepath)\n",
    "\n",
    "# Important Information Access\n",
    "#assert(meta_data['cnt3dYXZ'].shape[2] == len(slice_filepath) - 1)\n",
    "assert(pixel_data.shape[0] == raw_data.Rows)\n",
    "assert(pixel_data.shape[1] == raw_data.Columns)\n",
    "print(f\"Patient Position: {str(raw_data[0x0018, 0x5100].value)}\")\n",
    "if patient_pos == 'External':\n",
    "    print(f\"Number of Slices: {int(len(slice_list))}\")\n",
    "    print(f\"Slice Number: {int(raw_data[0x0020, 0x0013].value)}\")\n",
    "else:\n",
    "    print(f\"Number of Slices: {int(raw_data[0x2001, 0x1018].value)}\")\n",
    "    print(f\"Slice Number: {int(raw_data[0x2001, 0x100a].value)}\")\n",
    "    print(f\"Series Number: {int(raw_data.SeriesNumber)}\")\n",
    "\n",
    "#print(meta_data['cnt3dYXZ'].shape)\n",
    "print(pixel_data.shape)\n",
    "plt.imshow(pixel_data, cmap = plt.cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Positoning: FFP ; FFP\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "FFP: (0008, 0012) Instance Creation Date              DA: '20170321'\n",
      "OFP: (0008, 0012) Instance Creation Date              DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0013) Instance Creation Time              TM: '171934.980'\n",
      "OFP: (0008, 0013) Instance Creation Time              TM: '175555.355'\n",
      "\n",
      "FFP: (0008, 0018) SOP Instance UID                    UI: 1.3.46.670589.11.71641.5.0.1828.2017032116111778586\n",
      "OFP: (0008, 0018) SOP Instance UID                    UI: 1.3.46.670589.11.71641.5.0.4680.2017071816470843561\n",
      "\n",
      "FFP: (0008, 0020) Study Date                          DA: '20170321'\n",
      "OFP: (0008, 0020) Study Date                          DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0021) Series Date                         DA: '20170321'\n",
      "OFP: (0008, 0021) Series Date                         DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0022) Acquisition Date                    DA: '20170321'\n",
      "OFP: (0008, 0022) Acquisition Date                    DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0023) Content Date                        DA: '20170321'\n",
      "OFP: (0008, 0023) Content Date                        DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0030) Study Time                          TM: '160418'\n",
      "OFP: (0008, 0030) Study Time                          TM: '163911'\n",
      "\n",
      "FFP: (0008, 0031) Series Time                         TM: '160713.93000'\n",
      "OFP: (0008, 0031) Series Time                         TM: '164253.65000'\n",
      "\n",
      "FFP: (0008, 0032) Acquisition Time                    TM: '160722.82'\n",
      "OFP: (0008, 0032) Acquisition Time                    TM: '164302.40'\n",
      "\n",
      "FFP: (0008, 0033) Content Time                        TM: '160722.82'\n",
      "OFP: (0008, 0033) Content Time                        TM: '164302.40'\n",
      "\n",
      "FFP: (0008, 0050) Accession Number                    SH: '2017017868'\n",
      "OFP: (0008, 0050) Accession Number                    SH: '2017044005'\n",
      "\n",
      "FFP: (0008, 1111) Referenced Performed Procedure Step SQ: <Sequence, length 1>\n",
      "OFP: (0008, 1111) Referenced Performed Procedure Step SQ: <Sequence, length 1>\n",
      "\n",
      "FFP: (0008, 0012) Instance Creation Date              DA: '20170321'\n",
      "OFP: (0008, 0012) Instance Creation Date              DA: '20170718'\n",
      "\n",
      "FFP: (0008, 0013) Instance Creation Time              TM: '160418.395'\n",
      "OFP: (0008, 0013) Instance Creation Time              TM: '163911.905'\n",
      "\n",
      "FFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.11376.2017032116041839003\n",
      "OFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.10984.2017071816391190017\n",
      "\n",
      "FFP: (0008, 1140) Referenced Image Sequence           SQ: <Sequence, length 3>\n",
      "OFP: (0008, 1140) Referenced Image Sequence           SQ: <Sequence, length 3>\n",
      "\n",
      "FFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.1828.2017032116054257558\n",
      "OFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.4680.2017071816404009541\n",
      "\n",
      "FFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.1828.2017032116054257557\n",
      "OFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.4680.2017071816404006533\n",
      "\n",
      "FFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.1828.2017032116054258562\n",
      "OFP: (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.11.71641.5.0.4680.2017071816404009538\n",
      "\n",
      "FFP: (0010, 1030) Patient's Weight                    DS: '65.0'\n",
      "OFP: (0010, 1030) Patient's Weight                    DS: '64.0'\n",
      "\n",
      "FFP: (0018, 0080) Repetition Time                     DS: '702.870971679687'\n",
      "OFP: (0018, 0080) Repetition Time                     DS: '632.583923339843'\n",
      "\n",
      "FFP: (0018, 0084) Imaging Frequency                   DS: '127.763209'\n",
      "OFP: (0018, 0084) Imaging Frequency                   DS: '127.762929'\n",
      "\n",
      "FFP: (0018, 1020) Software Versions                   LO: ['5.3.0', '5.3.0.2']\n",
      "OFP: (0018, 1020) Software Versions                   LO: ['5.3.0', '5.3.0.3']\n",
      "\n",
      "FFP: (0018, 1316) SAR                                 DS: '1.97129201889038'\n",
      "OFP: (0018, 1316) SAR                                 DS: '1.95607185363769'\n",
      "\n",
      "FFP: (0018, 1318) dB/dt                               DS: '80.8085250854492'\n",
      "OFP: (0018, 1318) dB/dt                               DS: '81.0862731933593'\n",
      "\n",
      "FFP: (0018, 9073) Acquisition Duration                FD: 231.94741821289062\n",
      "OFP: (0018, 9073) Acquisition Duration                FD: 243.54481506347656\n",
      "\n",
      "FFP: (0020, 000d) Study Instance UID                  UI: 1.3.6.1.4.1.23849.23212228212228292729\n",
      "OFP: (0020, 000d) Study Instance UID                  UI: 1.3.6.1.4.1.23849.23212228212525212126\n",
      "\n",
      "FFP: (0020, 000e) Series Instance UID                 UI: 1.3.46.670589.11.71641.5.0.1828.2017032116071393583\n",
      "OFP: (0020, 000e) Series Instance UID                 UI: 1.3.46.670589.11.71641.5.0.4680.2017071816425367559\n",
      "\n",
      "FFP: (0020, 0010) Study ID                            SH: '537984258'\n",
      "OFP: (0020, 0010) Study ID                            SH: '537723551'\n",
      "\n",
      "FFP: (0020, 0013) Instance Number                     IS: '60'\n",
      "OFP: (0020, 0013) Instance Number                     IS: '56'\n",
      "\n",
      "FFP: (0020, 0032) Image Position (Patient)            DS: [-168.25151556318, -118.42232203483, 105.726453141914]\n",
      "OFP: (0020, 0032) Image Position (Patient)            DS: [-169.04140022347, -140.06561541557, 104.607093810501]\n",
      "\n",
      "FFP: (0020, 0037) Image Orientation (Patient)         DS: [0.99999564886093, 0, -0.0029566788580, 0, 1, 0]\n",
      "OFP: (0020, 0037) Image Orientation (Patient)         DS: [0.99997270107269, 0, 0.00739033799618, 0, 1, 0]\n",
      "\n",
      "FFP: (0020, 0052) Frame of Reference UID              UI: 1.3.46.670589.11.71641.5.0.4524.2017032116034591003\n",
      "OFP: (0020, 0052) Frame of Reference UID              UI: 1.3.46.670589.11.71641.5.0.9344.2017071816381109020\n",
      "\n",
      "FFP: (0020, 1041) Slice Location                      DS: '-105.22852532422'\n",
      "OFP: (0020, 1041) Slice Location                      DS: '-105.85351017424'\n",
      "\n",
      "FFP: (0028, 1053) Rescale Slope                       DS: '4.07619047619047'\n",
      "OFP: (0028, 1053) Rescale Slope                       DS: '5.18876678876678'\n",
      "\n",
      "FFP: (0040, 0244) Performed Procedure Step Start Date DA: '20170321'\n",
      "OFP: (0040, 0244) Performed Procedure Step Start Date DA: '20170718'\n",
      "\n",
      "FFP: (0040, 0245) Performed Procedure Step Start Time TM: '160418'\n",
      "OFP: (0040, 0245) Performed Procedure Step Start Time TM: '163911'\n",
      "\n",
      "FFP: (0040, 0250) Performed Procedure Step End Date   DA: '20170321'\n",
      "OFP: (0040, 0250) Performed Procedure Step End Date   DA: '20170718'\n",
      "\n",
      "FFP: (0040, 0251) Performed Procedure Step End Time   TM: '160418'\n",
      "OFP: (0040, 0251) Performed Procedure Step End Time   TM: '163911'\n",
      "\n",
      "FFP: (0040, 0253) Performed Procedure Step ID         SH: '537984258'\n",
      "OFP: (0040, 0253) Performed Procedure Step ID         SH: '537723551'\n",
      "\n",
      "FFP: (0040, 0275) Request Attributes Sequence         SQ: <Sequence, length 1>\n",
      "OFP: (0040, 0275) Request Attributes Sequence         SQ: <Sequence, length 1>\n",
      "\n",
      "FFP: (0008, 0050) Accession Number                    SH: '2017017868'\n",
      "OFP: (0008, 0050) Accession Number                    SH: '2017044005'\n",
      "\n",
      "FFP: (0040, 0009) Scheduled Procedure Step ID         SH: '2017017868'\n",
      "OFP: (0040, 0009) Scheduled Procedure Step ID         SH: '2017044005'\n",
      "\n",
      "FFP: (0040, 1001) Requested Procedure ID              SH: '2017017868'\n",
      "OFP: (0040, 1001) Requested Procedure ID              SH: '2017044005'\n",
      "\n",
      "FFP: (0040, 1001) Requested Procedure ID              SH: '2017017868'\n",
      "OFP: (0040, 1001) Requested Procedure ID              SH: '2017044005'\n",
      "\n",
      "FFP: (0040, 2004) Issue Date of Imaging Service Reque DA: '20170321'\n",
      "OFP: (0040, 2004) Issue Date of Imaging Service Reque DA: '20170718'\n",
      "\n",
      "FFP: (0040, 2005) Issue Time of Imaging Service Reque TM: '160418.380'\n",
      "OFP: (0040, 2005) Issue Time of Imaging Service Reque TM: '163911.881'\n",
      "\n",
      "FFP: (2001, 100a) [Slice Number MR]                   IS: '60'\n",
      "OFP: (2001, 100a) [Slice Number MR]                   IS: '56'\n",
      "\n",
      "FFP: (2001, 1018) [Number of Slices MR]               SL: 60\n",
      "OFP: (2001, 1018) [Number of Slices MR]               SL: 63\n",
      "\n",
      "FFP: (2001, 1022) [Water Fat Shift]                   FL: 0.9965006113052368\n",
      "OFP: (2001, 1022) [Water Fat Shift]                   FL: 0.9964996576309204\n",
      "\n",
      "FFP: (2001, 105f) [Stack Sequence]                    SQ: <Sequence, length 1>\n",
      "OFP: (2001, 105f) [Stack Sequence]                    SQ: <Sequence, length 1>\n",
      "\n",
      "FFP: (2001, 102d) [Number of Stack Slices]            SS: 60\n",
      "OFP: (2001, 102d) [Number of Stack Slices]            SS: 63\n",
      "\n",
      "FFP: (2005, 1071) [Unknown]                           FL: 0.16940546035766602\n",
      "OFP: (2005, 1071) [Unknown]                           FL: -0.42343902587890625\n",
      "\n",
      "FFP: (2005, 1075) [Unknown]                           FL: 180.0\n",
      "OFP: (2005, 1075) [Unknown]                           FL: 189.0\n",
      "\n",
      "FFP: (2005, 1078) [Unknown]                           FL: 50.90739059448242\n",
      "OFP: (2005, 1078) [Unknown]                           FL: 29.264097213745117\n",
      "\n",
      "FFP: (2005, 1079) [Unknown]                           FL: 16.72618865966797\n",
      "OFP: (2005, 1079) [Unknown]                           FL: 33.86046600341797\n",
      "\n",
      "FFP: (2005, 143c) [Unknown]                           FL: -17.200000762939453\n",
      "OFP: (2005, 143c) [Unknown]                           FL: -34.20000076293945\n",
      "\n",
      "FFP: (2001, 1083) [Imaging Frequency]                 DS: '127.763209'\n",
      "OFP: (2001, 1083) [Imaging Frequency]                 DS: '127.762929'\n",
      "\n",
      "FFP: (2005, 1000) [Unknown]                           FL: 0.16940546035766602\n",
      "OFP: (2005, 1000) [Unknown]                           FL: -0.42343902587890625\n",
      "\n",
      "FFP: (2005, 1008) [Unknown]                           FL: 50.90739059448242\n",
      "OFP: (2005, 1008) [Unknown]                           FL: 29.264097213745117\n",
      "\n",
      "FFP: (2005, 1009) [Unknown]                           FL: 105.22579956054688\n",
      "OFP: (2005, 1009) [Unknown]                           FL: 105.8584976196289\n",
      "\n",
      "FFP: (2005, 100a) [Unknown]                           FL: 1.0774602890014648\n",
      "OFP: (2005, 100a) [Unknown]                           FL: 0.2836898863315582\n",
      "\n",
      "FFP: (2005, 100b) [Unknown]                           FL: 2016.2540283203125\n",
      "OFP: (2005, 100b) [Unknown]                           FL: 1802.69384765625\n",
      "\n",
      "FFP: (2005, 100e) [Unknown]                           FL: 0.2659429609775543\n",
      "OFP: (2005, 100e) [Unknown]                           FL: 0.26158639788627625\n",
      "\n",
      "FFP: (2005, 102a) [Unknown]                           IS: '556301035'\n",
      "OFP: (2005, 102a) [Unknown]                           IS: '566757501'\n",
      "\n",
      "FFP: (2005, 1030) [Repetition Time]                   FL: [702.8709716796875, 0.0]\n",
      "OFP: (2005, 1030) [Repetition Time]                   FL: [632.5839233398438, 0.0]\n",
      "\n",
      "FFP: (2005, 1033) [Acquisition Duration]              FL: 231.94741821289062\n",
      "OFP: (2005, 1033) [Acquisition Duration]              FL: 243.54481506347656\n",
      "\n",
      "FFP: (2005, 140a) [Unknown]                           DS: '4.07619047619047'\n",
      "OFP: (2005, 140a) [Unknown]                           DS: '5.18876678876678'\n",
      "\n",
      "FFP: (2005, 140f) [Unknown]                           SQ: <Sequence, length 1>\n",
      "OFP: (2005, 140f) [Unknown]                           SQ: <Sequence, length 1>\n",
      "\n",
      "FFP: (0008, 002a) Acquisition DateTime                DT: '20170321'\n",
      "OFP: (0008, 002a) Acquisition DateTime                DT: '20170718'\n",
      "\n",
      "FFP: (0018, 9181) Specific Absorption Rate Value      FD: 1.9712920188903809\n",
      "OFP: (0018, 9181) Specific Absorption Rate Value      FD: 1.9560718536376953\n",
      "\n",
      "FFP: (0018, 9182) Gradient Output                     FD: 80.80852508544922\n",
      "OFP: (0018, 9182) Gradient Output                     FD: 81.08627319335938\n",
      "\n",
      "FFP: (2005, 142b) [Unknown]                           CS: 'INITIAL'\n",
      "OFP: (2005, 142b) [Unknown]                           CS: 'PARTLY_ACCEPTED'\n",
      "\n",
      "FFP: (2005, 1442) [Unknown]                           FL: 1.9798812866210938\n",
      "OFP: (2005, 1442) [Unknown]                           FL: 1.9798811674118042\n",
      "\n",
      "FFP: (2005, 1492) Private tag data                    FL: 0.45723608136177063\n",
      "OFP: (2005, 1492) Private tag data                    FL: 0.47639113664627075\n",
      "\n",
      "FFP: (7fe0, 0010) Pixel Data                          OW: Array of 1036800 elements\n",
      "OFP: (7fe0, 0010) Pixel Data                          OW: Array of 1036800 elements\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FFP vs OFP Comparison\n",
    "ffp_list = os.listdir(f'FFP/ID{patient_id}'); ofp_list = os.listdir(f'OFP/ID{patient_id}')\n",
    "ffp_filepath = f\"FFP/ID{patient_id}/{ffp_list[num_slice]}\"\n",
    "ofp_filepath = f\"OFP/ID{patient_id}/{ofp_list[num_slice]}\"\n",
    "ffp_data = pydicom.dcmread(ffp_filepath); ofp_data = pydicom.dcmread(ofp_filepath)\n",
    "#ffp_pixel = ffp_data.pixel_array; ofp_pixel = ofp_data.pixel_array\n",
    "print(f\"Patient Positoning: {str(ffp_data[0x0018, 0x5100].value)} ; {str(ofp_data[0x0018, 0x5100].value)}\\n\\n\" +\\\n",
    "                            \"-------------------------------------------------------------------------\\n\")\n",
    "for ffp, ofp in zip(ffp_data.iterall(), ofp_data.iterall()):\n",
    "    if ffp != ofp: print(f\"FFP: {ffp}\\nOFP: {ofp}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *MRI* **Visualiser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Visualizer\n",
    "def mri_visualizer(\n",
    "    num_slice: int = 0\n",
    "):\n",
    "\n",
    "    # Figure Initialization\n",
    "    figure = plt.figure(figsize = (10, 10))\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.imshow(meta_data['cnt3dYXZ'][:, :, num_slice])#, cmap = meta_data['cmap'])\n",
    "\n",
    "slice_slider = IntSlider(value = 0, min = 0,\n",
    "    max = meta_data['cnt3dYXZ'].shape[2] - 1,\n",
    "    description = 'Slice', continuous_update = False)\n",
    "interactive(mri_visualizer, num_slice = slice_slider)\n",
    "mri_visualizer(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b9acde1b2542d9958b52d0f560bf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Slice', max=59), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Image Visualizer\n",
    "def mri_visualizer(\n",
    "    num_slice: int = 0\n",
    "):\n",
    "\n",
    "    # Slice File Access\n",
    "    slice_filepath = f\"{patient_pos}/ID{patient_id}/{slice_list[num_slice]}\"\n",
    "    slice_data = pydicom.dcmread(slice_filepath)\n",
    "    pixel_data = slice_data.pixel_array\n",
    "    \n",
    "    # Figure Initialization\n",
    "    figure = plt.figure(figsize = (5, 5))\n",
    "    plt.title(f\"{str(slice_data[0x0018, 0x5100].value)} Position | \" +\\\n",
    "              f\"Slice #{int(slice_data[0x0020, 0x0013].value)} \" +\\\n",
    "              f\"out of {int(len(slice_list))}\")\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.imshow(pixel_data, cmap = plt.cm.binary)\n",
    "\n",
    "# Raw Data Access\n",
    "slice_list = os.listdir(f'{patient_pos}/ID{patient_id}')\n",
    "data_filepath = f\"{patient_pos}/ID{patient_id}/{slice_list[0]}\"\n",
    "raw_data = pydicom.dcmread(data_filepath)\n",
    "    \n",
    "# Slice Visualizer Initialization\n",
    "slice_slider = IntSlider(value = 0, min = 0,\n",
    "    max = int(len(slice_list) - 1),\n",
    "    description = 'Slice', continuous_update = True)\n",
    "interactive(mri_visualizer, num_slice = slice_slider)\n",
    "#mri_visualizer(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004ea7dc71cb4754bf5e68b52124a912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Slice', max=59), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dual Image Visualizer\n",
    "def mri_visualizer(\n",
    "    num_slice: int = 0\n",
    "):\n",
    "\n",
    "    # Slice File Access\n",
    "    ffp_filepath = f\"FFP/ID{patient_id}/{ffp_list[num_slice]}\"\n",
    "    ofp_filepath = f\"OFP/ID{patient_id}/{ofp_list[num_slice]}\"\n",
    "    ffp_data = pydicom.dcmread(ffp_filepath)\n",
    "    ofp_data = pydicom.dcmread(ofp_filepath)\n",
    "    ffp_pixel = ffp_data.pixel_array; ofp_pixel = ofp_data.pixel_array\n",
    "    \n",
    "    # Figure Initialization\n",
    "    figure = plt.figure(figsize = (10, 5))\n",
    "    plt.title(f\"FFP | Slice #{int(ffp_data[0x2001, 0x100a].value)} \" +\\\n",
    "              f\"out of {int(ffp_data[0x2001, 0x1018].value)}\")\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.subplot(1, 2, 1, title =    f\"FFP | Slice #{int(ffp_data[0x2001, 0x100a].value)} \" +\\\n",
    "                                    f\"out of {int(ffp_data[0x2001, 0x1018].value)}\")\n",
    "    plt.imshow(ffp_pixel, cmap = plt.cm.binary)\n",
    "    plt.subplot(1, 2, 2, title =    f\"OFP | Slice #{int(ofp_data[0x2001, 0x100a].value)} \" +\\\n",
    "                                    f\"out of {int(ofp_data[0x2001, 0x1018].value)}\")\n",
    "    plt.imshow(ofp_pixel, cmap = plt.cm.binary)\n",
    "\n",
    "# Raw Data Access\n",
    "ffp_list = os.listdir(f'FFP/ID{patient_id}')\n",
    "ofp_list = os.listdir(f'OFP/ID{patient_id}')\n",
    "data_filepath = f\"{patient_pos}/ID{patient_id}/{ffp_list[0]}\"\n",
    "raw_data = pydicom.dcmread(data_filepath)\n",
    "    \n",
    "# Slice Visualizer Initialization\n",
    "slice_slider = IntSlider(value = 0, min = 0,\n",
    "    max = int(raw_data[0x2001, 0x1018].value - 1),\n",
    "    description = 'Slice', continuous_update = True)\n",
    "interactive(mri_visualizer, num_slice = slice_slider)\n",
    "#mri_visualizer(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Dataset* **Reader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Conditional MetaBrest Dataset Reader Class (V0)\n",
    "class NCDataset(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,\n",
    "        dataset: str = 'private',\n",
    "        mode: str = 'train',\n",
    "    ):  \n",
    "        \n",
    "        # Dataset Choice\n",
    "        super(NCDataset).__init__(); self.settings = settings\n",
    "        self.mode = mode; self.dataset = dataset\n",
    "        if self.dataset == 'public': self.data_folderpath = self.settings.public_data_folderpath\n",
    "        elif self.dataset == 'private': self.data_folderpath = self.settings.private_data_folderpath\n",
    "        else: print(\"ERROR: Chosen Dataset / Directory does not exist!\")\n",
    "        \n",
    "        # Subject Indexing (Existing or New Version)\n",
    "        subj_listpath = Path(f\"{self.settings.data_reader_folderpath}/V{self.settings.data_version}\" +\\\n",
    "                             f\"/{self.dataset}_{self.mode}_setV{self.settings.data_version}.txt\")\n",
    "        if subj_listpath.exists():\n",
    "            print(f\"Reading {self.dataset} Dataset Save Files for {self.mode} Set | Version {settings.data_version}\")\n",
    "            self.subj_list = subj_listpath.read_text().splitlines()\n",
    "        else:\n",
    "            print(f\"Generating New Save Files for {self.dataset} Dataset | Version {settings.data_version}\")\n",
    "            self.subj_list = os.listdir(self.data_folderpath)       # Complete List of Subjects in Dataset\n",
    "\n",
    "            self.subj_list = self.subj_split(self.subj_list)        # Selected List of Subjects in Dataset\n",
    "        #assert len(self.subj_list) == self.num_subj, f\"WARNING: Number of subjs does not match Dataset Version!\"\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Dataset Transformations Initialization\n",
    "        self.transform = transforms.Compose([   transforms.Resize(( self.settings.img_size,\n",
    "                                                            self.settings.img_size)),\n",
    "                                                transforms.ToTensor()])\n",
    "        self.h_flip = transforms.Compose([      transforms.RandomHorizontalFlip(p = 1)])\n",
    "        self.v_flip = transforms.Compose([      transforms.RandomVerticalFlip(p = 1)])\n",
    "\n",
    "    # ============================================================================================\n",
    "\n",
    "    # DataLoader Length / No. Subjects Computation Functionality\n",
    "    def __len__(self): len(self.subj_list)\n",
    "    \n",
    "    # Subject Splitting Functionality\n",
    "    def subj_split(self, subj_list: list):\n",
    "\n",
    "        # Dataset Splitting\n",
    "        assert 0 < (self.settings.train_subj + self.settings.val_subj + self.settings.test_subj) <= len(subj_list),\\\n",
    "               f\"ERROR: Dataset does not contain {self.settings.train_subj + self.settings.val_subj + self.settings.test_subj} Subjects!\"\n",
    "        train_subj = np.sort(np.array(random.sample(subj_list, self.settings.train_subj), dtype = 'str'))\n",
    "        subj_list = [subj for subj in subj_list if subj not in train_subj]                                  # Training Set Splitting\n",
    "        val_subj = np.sort(np.array(random.sample(subj_list, self.settings.val_subj), dtype = 'str'))\n",
    "        subj_list = [subj for subj in subj_list if subj not in val_subj]                                    # Validation Set Splitting\n",
    "        test_subj = np.sort(np.array(random.sample(subj_list, self.settings.test_subj), dtype = 'str'))\n",
    "        subj_list = [subj for subj in subj_list if subj not in test_subj]                                   # Test Set Splitting\n",
    "        subj_list = np.sort(np.array(subj_list, dtype = 'str'))\n",
    "        assert len(subj_list) + self.settings.train_subj + self.settings.val_subj + self.settings.test_subj == len(self.subj_list),\\\n",
    "               f\"ERROR: Dataset Splitting went Wrong!\"\n",
    "\n",
    "        # Dataset Split Saving\n",
    "        if not os.path.isdir(f\"V{self.settings.data_version}\"): os.mkdir(f\"V{self.settings.data_version}\")\n",
    "        if len(train_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_train_setV{self.settings.data_version}.txt\", train_subj, fmt='%s')\n",
    "        if len(val_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_val_setV{self.settings.data_version}.txt\", val_subj, fmt='%s')\n",
    "        if len(test_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_test_setV{self.settings.data_version}.txt\", test_subj, fmt='%s')\n",
    "        if len(subj_list) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_rest_set (V{self.settings.data_version}).txt\", subj_list, fmt='%s')\n",
    "        \n",
    "    # ============================================================================================\n",
    "        \n",
    "    # Single Batch / Subject Generation Functionality\n",
    "    def __getitem__(self, idx: int = 0 or str):\n",
    "        \n",
    "        # Subject Folder Access\n",
    "        subj_idx = idx if type(idx) == str else self.subj_list[idx]\n",
    "        subj_folderpath = f\"{self.data_folderpath}/{subj_idx}\"\n",
    "        subj_filelist = os.listdir(subj_folderpath); i = 0\n",
    "        while len(subj_filelist) > 0 and len(subj_filelist) <= 3:\n",
    "            subj_folderpath = Path(f\"{subj_folderpath}/{subj_filelist[0]}\")\n",
    "            subj_filelist = os.listdir(subj_folderpath)\n",
    "        \n",
    "        while os.path.splitext(f\"{subj_folderpath}/{subj_filelist[i]}\")[1] not in ['', '.dcm']: i += 1\n",
    "\n",
    "        # Subject General Information Access\n",
    "        subj_filepath = Path((f\"{subj_folderpath}/{subj_filelist[i]}\"))\n",
    "        subj_info = pydicom.dcmread(subj_filepath)\n",
    "        subj_id = str(subj_info[0x0010, 0x0010].value)\n",
    "        subj_ori = subj_info[0x0020, 0x0037].value\n",
    "        subj_v_flip = (np.all(subj_ori == [-1, 0, 0, 0, -1, 0]))\n",
    "        subj_h_flip = (torch.rand(1) < (self.settings.h_flip / 100))\n",
    "        num_row = subj_info.Rows; num_col = subj_info.Columns\n",
    "        if self.dataset == 'private':\n",
    "            num_slice = int(subj_info[0x2001, 0x1018].value)\n",
    "            preg_status = int(subj_info[0x0010, 0x21c0].value)\n",
    "        else:\n",
    "            num_slice = int(subj_info[0x0020, 0x1002].value)\n",
    "            preg_status = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "            \n",
    "        # Subject Slice Data Access\n",
    "        img_data = torch.empty((num_slice, self.settings.img_size, self.settings.img_size))\n",
    "        for slice_filepath in subj_filelist:\n",
    "            if os.path.splitext(slice_filepath)[1] in ['', '.dcm']:\n",
    "                \n",
    "                # Slice Data Access\n",
    "                slice_filepath = Path(f\"{subj_folderpath}/{slice_filepath}\")\n",
    "                slice_data = pydicom.dcmread(slice_filepath, force=True)\n",
    "                slice_idx = int(slice_data[0x0020, 0x0013].value) - 1\n",
    "                img_slice = slice_data.pixel_array.astype(float)\n",
    "\n",
    "                # Slice Image Pre-Processing | Rescaling, Resizing & Flipping\n",
    "                if self.settings.data_prep:\n",
    "                    img_slice = np.uint8((np.maximum(img_slice, 0) / img_slice.max()) * 255)\n",
    "                    img_slice = Image.fromarray(img_slice).resize(( self.settings.img_size,\n",
    "                                                                    self.settings.img_size)) \n",
    "                    if subj_h_flip: img_slice = self.h_flip(img_slice)\n",
    "                    if subj_v_flip: img_slice = self.v_flip(img_slice)\n",
    "                    img_slice = np.array(self.transform(img_slice))\n",
    "                img_data[slice_idx, :, :] = torch.Tensor(img_slice)\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Correction for Chosen Number of Slices\n",
    "        extra_slice = self.settings.num_slice - num_slice\n",
    "        if num_slice < self.settings.num_slice:             # Addition of Repeated Peripheral Slices\n",
    "            for extra in range(extra_slice):\n",
    "                if extra % 2 == 0: img_data = torch.cat((img_data, img_data[-1].unsqueeze(0)), dim = 0)\n",
    "                else: img_data = torch.cat((img_data[0].unsqueeze(0), img_data), dim = 0)\n",
    "        elif num_slice > self.settings.num_slice:           # Removal of Emptier Peripheral Slices\n",
    "            img_data = img_data[int(np.ceil(-extra_slice / 2)) :\\\n",
    "                int(len(img_data) - np.floor(-extra_slice / 2))]\n",
    "        else: assert(num_slice == self.settings.num_slice)\n",
    "          \n",
    "        # Item Dictionary Returning\n",
    "        return {'img_data': img_data,#.unsqueeze(0),\n",
    "                'resolution': f'[{num_row}, {num_col}]',\n",
    "                'subj_id': subj_id, 'num_slice': num_slice,\n",
    "                'preg_status': preg_status, 'orientation': subj_ori,\n",
    "                'h_flip': subj_h_flip, 'v_flip': subj_v_flip,\n",
    "                'position': str(subj_info[0x0018, 0x5100].value),\n",
    "                'num_series': int(subj_info.SeriesNumber)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Conditional MetaBrest Dataset Reader Class (V1)\n",
    "class NCDataset(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,\n",
    "        dataset: str = 'private',\n",
    "        mode: str = 'train',\n",
    "    ):  \n",
    "        \n",
    "        # Dataset Choice\n",
    "        super(NCDataset).__init__(); self.settings = settings\n",
    "        self.mode = mode; self.dataset = dataset\n",
    "        if self.dataset == 'public': self.data_folderpath = self.settings.public_data_folderpath\n",
    "        elif self.dataset == 'private': self.data_folderpath = self.settings.private_data_folderpath\n",
    "        else: print(\"ERROR: Chosen Dataset / Directory does not exist!\")\n",
    "        \n",
    "        # Subject Indexing (Existing or New Version)\n",
    "        subj_listpath = Path(f\"{self.settings.reader_folderpath}/V{self.settings.data_version}\" +\\\n",
    "                             f\"/{self.dataset}_{self.mode}_setV{self.settings.data_version}.txt\")\n",
    "        if subj_listpath.exists():\n",
    "            print(f\"Reading {self.dataset} Dataset Save Files for {self.mode} Set | Version {settings.data_version}\")\n",
    "            self.subj_list = subj_listpath.read_text().splitlines()\n",
    "        else:\n",
    "            print(f\"Generating New Save Files for {self.dataset} Dataset | Version {settings.data_version}\")\n",
    "            self.subj_list = os.listdir(self.data_folderpath)       # Complete List of Subjects in Dataset\n",
    "            self.subj_list = self.subj_split(self.subj_list)        # Selected List of Subjects in Dataset\n",
    "        #assert len(self.subj_list) == self.num_subj, f\"WARNING: Number of subjs does not match Dataset Version!\"\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Dataset Transformations Initialization\n",
    "        self.transform = transforms.Compose([\n",
    "                                        transforms.Resize(( self.settings.img_size,\n",
    "                                                            self.settings.img_size)),\n",
    "                                        transforms.ToTensor()])\n",
    "        self.h_flip = transforms.Compose([transforms.RandomHorizontalFlip(p = 1)])\n",
    "        self.v_flip = transforms.Compose([transforms.RandomVerticalFlip(p = 1)])\n",
    "\n",
    "    # ============================================================================================\n",
    "\n",
    "    # DataLoader Length / No. Subjects Computation Functionality\n",
    "    def __len__(self): return len(self.subj_list)\n",
    "    \n",
    "    # Subject Splitting Functionality\n",
    "    def subj_split(self, subj_list: list):\n",
    "\n",
    "        # Dataset Splitting\n",
    "        assert 0 < (self.settings.train_subj + self.settings.val_subj + self.settings.test_subj) <= len(subj_list),\\\n",
    "               f\"ERROR: Dataset does not contain {self.settings.train_subj + self.settings.val_subj + self.settings.test_subj} Subjects!\"\n",
    "        if self.settings.train_subj != 0:\n",
    "            train_subj = np.sort(np.array(random.sample(subj_list, self.settings.train_subj), dtype = 'str'))\n",
    "            subj_list = [subj for subj in subj_list if subj not in train_subj]                                  # Training Set Splitting\n",
    "            val_subj = np.sort(np.array(random.sample(subj_list, self.settings.val_subj), dtype = 'str'))\n",
    "            subj_list = [subj for subj in subj_list if subj not in val_subj]                                    # Validation Set Splitting\n",
    "            test_subj = np.sort(np.array(random.sample(subj_list, self.settings.test_subj), dtype = 'str'))\n",
    "            subj_list = [subj for subj in subj_list if subj not in test_subj]                                   # Test Set Splitting\n",
    "            subj_list = np.sort(np.array(subj_list, dtype = 'str'))\n",
    "        else: train_subj = np.sort(np.array(subj_list, dtype = 'str'))\n",
    "        assert len(subj_list) + self.settings.train_subj + self.settings.val_subj + self.settings.test_subj == len(self.subj_list),\\\n",
    "                                                                                            f\"ERROR: Dataset Splitting went Wrong!\"\n",
    "\n",
    "        # Dataset Split Saving\n",
    "        if not os.path.isdir(f\"V{self.settings.data_version}\"): os.mkdir(f\"V{self.settings.data_version}\")\n",
    "        if len(train_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_train_setV{self.settings.data_version}.txt\", train_subj, fmt='%s')\n",
    "        if len(val_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_val_setV{self.settings.data_version}.txt\", val_subj, fmt='%s')\n",
    "        if len(test_subj) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_test_setV{self.settings.data_version}.txt\", test_subj, fmt='%s')\n",
    "        if len(subj_list) != 0: np.savetxt(f\"V{self.settings.data_version}/{self.dataset}_rest_set (V{self.settings.data_version}).txt\", subj_list, fmt='%s')\n",
    "        \n",
    "    # ============================================================================================\n",
    "        \n",
    "    # Single Batch / Subject Generation Functionality\n",
    "    def __getitem__(self, idx: int = 0 or str):\n",
    "        \n",
    "        # Subject Folder Access\n",
    "        subj_idx = idx if type(idx) == str else self.subj_list[idx]\n",
    "        subj_folderpath = f\"{self.data_folderpath}/{subj_idx}\"\n",
    "        subj_filelist = os.listdir(subj_folderpath); i = 0\n",
    "        while len(subj_filelist) > 0 and len(subj_filelist) <= 3:\n",
    "            while os.path.splitext(f\"{subj_folderpath}/{subj_filelist[i]}\")[1] not in ['', '.dcm']: i += 1\n",
    "            subj_folderpath = Path(f\"{subj_folderpath}/{subj_filelist[i]}\")\n",
    "            subj_filelist = os.listdir(subj_folderpath); i = 0\n",
    "        while os.path.splitext(f\"{subj_folderpath}/{subj_filelist[i]}\")[1] not in ['', '.dcm']: i += 1\n",
    "\n",
    "        # Subject General Information Access\n",
    "        #print(f\"Accessing Subject {subj_idx}: {len(subj_filelist) - i} Slices\")\n",
    "        subj_filepath = Path((f\"{subj_folderpath}/{subj_filelist[i]}\"))\n",
    "        subj_info = pydicom.dcmread(subj_filepath)\n",
    "        subj_ori = subj_info[0x0020, 0x0037].value\n",
    "        subj_v_flip = (np.all(subj_ori == [-1, 0, 0, 0, -1, 0]))\n",
    "        subj_h_flip = (torch.rand(1) < (self.settings.h_flip / 100))\n",
    "        #num_row = subj_info.Rows; num_col = subj_info.Columns\n",
    "        #if self.dataset == 'private':\n",
    "            #num_slice = int(subj_info[0x2001, 0x1018].value)\n",
    "            #preg_status = int(subj_info[0x0010, 0x21c0].value)\n",
    "        #else:\n",
    "            #num_slice = int(subj_info[0x0020, 0x1002].value)\n",
    "            #preg_status = None\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "            \n",
    "        # Subject Slice Data Access\n",
    "        img_data = torch.empty((70, self.settings.img_size, self.settings.img_size)); slice_list = []\n",
    "        for slice_filepath in subj_filelist:\n",
    "            if os.path.splitext(slice_filepath)[1] in ['', '.dcm']:\n",
    "                \n",
    "                # Slice Data Access\n",
    "                slice_filepath = Path(f\"{subj_folderpath}/{slice_filepath}\")\n",
    "                slice_data = pydicom.dcmread(slice_filepath, force=True)\n",
    "                slice_idx = int(slice_data[0x0020, 0x0013].value) - 1\n",
    "                slice_list.append(slice_idx)\n",
    "                img_slice = slice_data.pixel_array.astype(float)\n",
    "\n",
    "                # Slice Image Pre-Processing | Rescaling, Resizing & Flipping\n",
    "                if self.settings.data_prep:\n",
    "                    img_slice = np.uint8((np.maximum(img_slice, 0) / img_slice.max()) * 255)\n",
    "                    img_slice = Image.fromarray(img_slice).resize(( self.settings.img_size,\n",
    "                                                                    self.settings.img_size)) \n",
    "                    if subj_h_flip: img_slice = self.h_flip(img_slice)\n",
    "                    if subj_v_flip: img_slice = self.v_flip(img_slice)\n",
    "                    img_slice = np.array(self.transform(img_slice))\n",
    "                img_data[slice_idx, :, :] = torch.Tensor(img_slice); del img_slice\n",
    "        img_data = img_data[np.sort(slice_list)]\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Correction for Chosen Number of Slices\n",
    "        extra_slice = self.settings.num_slice - img_data.shape[0]\n",
    "        if img_data.shape[0] < self.settings.num_slice:             # Addition of Repeated Peripheral Slices\n",
    "            for extra in range(extra_slice):\n",
    "                if extra % 2 == 0: img_data = torch.cat((img_data, img_data[-1].unsqueeze(0)), dim = 0)\n",
    "                else: img_data = torch.cat((img_data[0].unsqueeze(0), img_data), dim = 0)\n",
    "        elif img_data.shape[0] > self.settings.num_slice:           # Removal of Emptier Peripheral Slices\n",
    "            img_data = img_data[int(np.ceil(-extra_slice / 2)) :\\\n",
    "                int(len(img_data) - np.floor(-extra_slice / 2))]\n",
    "        #else: assert(num_slice == self.settings.num_slice)\n",
    "          \n",
    "        # Item Dictionary Returning\n",
    "        return img_data.unsqueeze(0)\n",
    "        \"\"\"return {'img_data': img_data,#.unsqueeze(0),\n",
    "                'resolution': f'[{num_row}, {num_col}]',\n",
    "                'subj_id': subj_id, 'num_slice': num_slice,\n",
    "                'preg_status': preg_status, 'orientation': subj_ori,\n",
    "                'h_flip': subj_h_flip, 'v_flip': subj_v_flip,\n",
    "                'position': str(subj_info[0x0018, 0x5100].value),\n",
    "                'num_series': int(subj_info.SeriesNumber)}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading private Dataset Save Files for train Set | Version 0\n",
      "torch.Size([1, 30, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Dataset Initialization Example\n",
    "dataset = NCDataset(settings,\n",
    "                    mode = 'train',\n",
    "                    dataset = 'private')\n",
    "data = dataset.__getitem__(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221c89fd3d884ab8b188b3c9c6a7854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Slice', max=30, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single 3D Image Visualizer\n",
    "def mri_visualizer(\n",
    "    num_slice: int = 0\n",
    "):\n",
    "    \n",
    "    # Figure Initialization\n",
    "    figure = plt.figure(figsize = (5, 5)); plt.title(f\"Slice #{num_slice}\")\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.imshow(data[0][num_slice - 1], cmap = plt.cm.binary)\n",
    "    \n",
    "# Slice Visualizer Initialization\n",
    "slice_slider = IntSlider(   value = 1, min = 1,\n",
    "                            max = settings.num_slice,\n",
    "    description = 'Slice', continuous_update = True)\n",
    "interactive(mri_visualizer, num_slice = slice_slider)\n",
    "#mri_visualizer(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Video** *Diffusion*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation** *Metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fréchet Inception Distance Score** (*FID Score*)\n",
    "\n",
    "- https://lightning.ai/docs/torchmetrics/stable/image/frechet_inception_distance.html\n",
    "- https://pytorch.org/torcheval/main/generated/torcheval.metrics.FrechetInceptionDistance.html\n",
    "- https://github.com/mseitzer/pytorch-fid/tree/master\n",
    "\n",
    "**3D Structural Similarity Index** (*3D SSIM Index*)\n",
    "\n",
    "- https://github.com/jinh0park/pytorch-ssim-3D/tree/master\n",
    "\n",
    "**Sorensen-Dice Coefficient** (*Dice Score*)\n",
    "\n",
    "- https://medium.com/@saba99/dice-or-dice-score-fa9f70422db4\n",
    "- https://copyprogramming.com/howto/dice-coefficient-image-segmentation-python\n",
    "\n",
    "**Inception Score** (*IS Loss*)\n",
    "\n",
    "- https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/\n",
    "- https://kailashahirwar.medium.com/a-very-short-introduction-to-inception-score-is-c9b03a7dd788\n",
    "\n",
    "**Normalized Mutual Information** (*NMI Loss*)\n",
    "\n",
    "**Peak Signal-to-Noise Ratio** (*PSNR Loss*)\n",
    "\n",
    "- https://scikit-image.org/docs/stable/api/skimage.metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.0.post0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics) (1.25.2)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\sousa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-fidelity in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (1.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (10.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (1.11.2)\n",
      "Requirement already satisfied: torch in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (0.16.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->torch-fidelity) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision->torch-fidelity) (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->torch-fidelity) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch->torch-fidelity) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision->torch-fidelity) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision->torch-fidelity) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision->torch-fidelity) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision->torch-fidelity) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch->torch-fidelity) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\sousa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics[image] in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.0.post0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (1.25.2)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (2.1.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (0.10.1)\n",
      "Requirement already satisfied: torchvision>=0.8 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (0.16.2)\n",
      "Requirement already satisfied: scipy>1.0.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (1.11.2)\n",
      "Requirement already satisfied: torch-fidelity<=0.4.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchmetrics[image]) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics[image]) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics[image]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics[image]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics[image]) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->torchmetrics[image]) (2023.10.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (10.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision>=0.8->torchmetrics[image]) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.10.0->torchmetrics[image]) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.10.0->torchmetrics[image]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sousa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->torch-fidelity<=0.4.0->torchmetrics[image]) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\sousa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ssim3d_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 3D SSIM Index Requisites (Pip Package)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#%pip install pytorch_ssim\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#import pytorch_ssim\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#from pytorch_ssim import ssim3D\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 3D SSIM Index Requisites (Local Package)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28meval\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mssim3d_metric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ssim3D\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Inception Score Requisites\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ssim3d_metric'"
     ]
    }
   ],
   "source": [
    "# FID Score Requisites\n",
    "%pip install torchmetrics\n",
    "%pip install torch-fidelity\n",
    "%pip install torchmetrics[image]\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance as FID\n",
    "\n",
    "# 3D SSIM Index Requisites (Pip Package)\n",
    "#%pip install pytorch_ssim\n",
    "#import pytorch_ssim\n",
    "#from pytorch_ssim import ssim3D\n",
    "\n",
    "# 3D SSIM Index Requisites (Local Package)\n",
    "sys.path.append('eval')\n",
    "from ssim3d_metric import ssim3D\n",
    "\n",
    "# Inception Score Requisites\n",
    "import keras\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "\n",
    "# Dice Score, PSNR  Requisites\n",
    "%pip install opencv-python\n",
    "import cv2\n",
    "%pip install scikit-image\n",
    "import skimage\n",
    "from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "from skimage.metrics import normalized_mutual_information as NMI\n",
    "from skimage.filters import threshold_li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Dice Score Functionality\n",
    "def dice_score(input_img, gen_img):\n",
    "\n",
    "    # Image Binarization (Li's Method)\n",
    "    #x_thresh, x = cv2.threshold(x, 128, 192, cv2.THRESH_OTSU)\n",
    "    #y_thresh, y = cv2.threshold(y, 128, 192, cv2.THRESH_OTSU)\n",
    "    input_img = input_img > threshold_li(input_img)\n",
    "    gen_img = gen_img > threshold_li(gen_img)\n",
    "    #print(x_thresh, y_thresh)\n",
    "\n",
    "    # Dice Score Computation\n",
    "    intersect = np.sum(input_img * gen_img)\n",
    "    if (np.sum(input_img) == 0) and (np.sum(gen_img) == 0): return 1\n",
    "    return (2 * intersect) / (np.sum(input_img) + np.sum(gen_img))\n",
    "\n",
    "# Mean Dice Score Functionality\n",
    "def mean_dice_score(input_img, gen_img):\n",
    "    assert(np.all(input_img.shape == gen_img.shape)); mean_score = 0.\n",
    "    if type(input_img) == torch.Tensor:\n",
    "        input_img = input_img.numpy(); gen_img = gen_img.numpy()\n",
    "    for i in range(input_img.shape[0]):\n",
    "        for j in range(input_img.shape[1]):\n",
    "            score = dice_score(input_img[i, j, :, :], gen_img[i, j, :, :])\n",
    "            mean_score += score / (input_img.shape[1] * input_img.shape[0])\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception Score Functionality (WIP)\n",
    "def inception_score(\n",
    "    inception_model,\n",
    "    gen_img,\n",
    "    num_split: int = 10,\n",
    "    eps: float = 1e-16\n",
    "):\n",
    "\n",
    "    # Inception Score Computation\n",
    "    y_hat = inception_model.predict(gen_img); is_score = list()\n",
    "    num_part = np.floor(gen_img.shape[0] / num_split)\n",
    "    for i in range(num_split):\n",
    "        idx_start, idx_end = i * num_part, (i * num_part) + num_part\n",
    "        prob_yx = y_hat[idx_start : idx_end]\n",
    "        prob_y = np.expand_dims(prob_yx.mean(axis = 0), 0)\n",
    "        kld_div = prob_yx * (np.log(prob_yx + eps) - np.log(prob_y + eps))\n",
    "        is_score.append(np.exp(np.mean(kld_div.sum(axis = 1))))\n",
    "    return np.mean(is_score), np.std(is_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 12.622090339660645\n",
      "3D SSIM Index: 0.002742826472967863\n",
      "Dice Score: 0.567898018767666\n",
      "PSNR Loss: 7.771684037037087\n",
      "NMI Loss: 1.0044343481596893\n"
     ]
    }
   ],
   "source": [
    "# FID Score Computation Example\n",
    "img1 = torch.randint(0, 200, (100, 3, 299, 299), dtype = torch.uint8)\n",
    "img2 = torch.randint(100, 255, (100, 3, 299, 299), dtype = torch.uint8)\n",
    "fid_metric = FID(feature = 64)\n",
    "fid_metric.update(img1, real = True)\n",
    "fid_metric.update(img2, real = False)\n",
    "print(f\"FID Score: {fid_metric.compute()}\")\n",
    "\n",
    "# 3D SSIM Index Computation Example\n",
    "img1 = torch.randn(1, 1, 30, 64, 64)\n",
    "img2 = torch.randn(1, 1, 30, 64, 64)\n",
    "print(f\"3D SSIM Index: {ssim3D(img1, img2)}\")\n",
    "\n",
    "# Binarization and Dice Score Computation Example\n",
    "img1 = torch.randn(1, 30, 64, 64)\n",
    "img2 = torch.randn(1, 30, 64, 64)\n",
    "print(f\"Dice Score: {mean_dice_score(img1, img2)}\")\n",
    "\n",
    "\"\"\"\n",
    "# Inception Score Computation Example (WIP)\n",
    "img2 = torch.randn(1, 64, 64, 3)\n",
    "inception_model = InceptionV3()\n",
    "is_mean, is_std = inception_score(inception_model, img2)\n",
    "print(f\"Inception Score: {is_mean} +- {is_std}\")\n",
    "\"\"\"\n",
    "\n",
    "#NMI & PSNR Loss Computation Examples\n",
    "img1 = torch.rand(1, 30, 64, 64).numpy()\n",
    "img2 = torch.rand(1, 30, 64, 64).numpy()\n",
    "print(f\"PSNR Loss: {PSNR(img1, img2)}\")\n",
    "print(f\"NMI Loss: {NMI(img1, img2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Blackout** *Diffusion*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_head = 8\n",
    "num_bucker = 32\n",
    "max_dist = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Architectural** *Layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Position Bias Layer Class\n",
    "class RelativePosBias(nn.Module):\n",
    "\n",
    "  # Description:  It is more effective than Positional Embeddings because it uses\n",
    "  #               Self-Attention to also encode the distance between any 2 tokens,\n",
    "  #               thus providing more flexibility and eliminating the need to input\n",
    "  #               this distance manually.\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "      self,\n",
    "      settings\n",
    "      #num_head: int = 8,\n",
    "      #num_bucket: int = 32,\n",
    "      #max_dist: int = 128\n",
    "  ):\n",
    "    super().__init__(); self.settings = settings\n",
    "    self.rel_attn_bias = nn.Embedding(self.settings.num_bucket, self.settings.num_head)\n",
    "\n",
    "  # --------------------------------------------------------------------------------------------\n",
    "\n",
    "  @static_method\n",
    "  def rel_pos_bucket(\n",
    "    rel_pos,\n",
    "    num_bucket: int = 32,\n",
    "    max_dist = 128\n",
    "  ):\n",
    "    ret = 0; num_bucket //= 2; n = -rel_pos; max_exact = num_bucket // 2\n",
    "    ret += (n < 0).long() * num_bucket; n = torch.abs(n)\n",
    "    value = max + torch.log(n.float() / max_exact) / math.log(max_dist\\\n",
    "                          / max_exact) * (num_bucket - max_exact)).long()\n",
    "    value = torch.min(value, torch.full_like(value, num_bucket - 1))\n",
    "    return ret + torch.where(n < max_exact, n, value)\n",
    "\n",
    "  # --------------------------------------------------------------------------------------------\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      n: int = 0\n",
    "  ):\n",
    "\n",
    "    # Relative Position Embeddings Computation\n",
    "    q = torch.arange(n, dtype = torch.long, device = self.settings.device)\n",
    "    k = torch.arange(n, dtype = torch.long, device = self.settings.device)\n",
    "    rp = rearrange(k, 'j -> 1 j') - rearrange(q, 'i -> i 1')\n",
    "    rp_bucket = self.rel_pos_bucket(rp, num_bucket = self.settings.num_bucket,\n",
    "                                        max_dist = self.settings.max_dist)\n",
    "    return rearrange(self.rel_attn_bias(rp_bucket), 'i j h -> h i j')\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Sinusoidal Position Embedding Layer Class\n",
    "class SinusoidalPosEmbedding(nn.Module):\n",
    "\n",
    "  # Description:  This Module will take as Input the Noise Levels of all the\n",
    "  #               Images in the Batch [batch_size, 1] and return a tensor\n",
    "  #               containing the Position Embeddings' Dimensionality\n",
    "  #               [batch_size, dim] so it can be added to the Residual Blocks.\n",
    "  # Source:       https://arxiv.org/abs/1706.03762\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "      self,\n",
    "      dim: int\n",
    "  ):  super().__init__(); self.dim = dim\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      ts: int = 0\n",
    "  ):\n",
    "\n",
    "    # Position Embeddings Computation\n",
    "    embed = math.log(10000) / ((self.dim // 2) - 1)\n",
    "    embed = torch.exp(torch.arange(self.dim // 2, device = ts.device) * (-embed))\n",
    "    embed = ts[:, None] * embed[None, :]\n",
    "    return torch.cat((embed.sin(), embed.cos()), dim = -1)\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Residual Connection Layer Class\n",
    "class Residual(nn.Module):\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "      self,\n",
    "      fn\n",
    "  ):  super().__init__(); self.fn = fn\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      x: torch.Tensor,\n",
    "      *args,\n",
    "      **kwargs\n",
    "  ): return self.fn(x, *args, **kwargs) + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling Layer Functionality\n",
    "def Downsample(dim: int):\n",
    "  return nn.Conv3d( dim, dim, kernel_size = (1, 4, 4),\n",
    "                    stride = (1, 2, 2), padding = (0, 1, 1))\n",
    "\n",
    "# Upsampling Layer Functionality\n",
    "def Upsample(dim: int):\n",
    "  return nn.ConvTranspose3d(dim, dim, kernel_size = (1, 4, 4),\n",
    "                            stride = (1, 2, 2), padding = (0, 1, 1))\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Helper Functionalities\n",
    "def default(val, d):\n",
    "  if exists(val): return val\n",
    "  return d() if isfunction(d) else d\n",
    "\n",
    "def exists(x): return x is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary Group Normalization Layer Class\n",
    "class PreNorm(nn.Module):\n",
    "\n",
    "  # Description:  Group Normalization is applied before Attention\n",
    "  #               although this is contentional in literature\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim: int,\n",
    "    fn\n",
    "  ):\n",
    "\n",
    "    # Layer Architecture Definition\n",
    "    super().__init__(); self.fn = fn\n",
    "    self.norm = LayerNorm(dim)\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      x,\n",
    "      **kwargs\n",
    "  ):  return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "# Preliminary Group Normalization Layer Class\n",
    "class PreNorm(nn.Module):\n",
    "\n",
    "  # Description:  Group Normalization is applied before Attention\n",
    "  #               although this is contentional in literature\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim: int,\n",
    "    fn\n",
    "  ):\n",
    "\n",
    "    # Layer Architecture Definition\n",
    "    super().__init__(); self.fn = fn\n",
    "    self.norm = LayerNorm(dim)\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      x,\n",
    "      **kwargs\n",
    "  ):  return self.fn(self.norm(x), **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer Class\n",
    "class Attention(nn.Module):\n",
    "\n",
    "  # Constructor / Initialization Function\n",
    "  def __init__(\n",
    "      self,\n",
    "      dim: int,\n",
    "      layer: str = 'quadratic',\n",
    "      num_head: int = 4,\n",
    "      head_dim: int = 32\n",
    "  ):\n",
    "\n",
    "    # Layer Architecture Definition\n",
    "    super().__init__(); self.layer = layer; self.num_head = num_head\n",
    "    self.scale = head_dim ** (-0.5); hidden_dim = head_dim * num_head\n",
    "    self.conv1 = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "    if self.layer == 'quadratic': self.conv2 = nn.Conv2d(hidden_dim, dim, 1)\n",
    "    else: self.conv2 = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), nn.GroupNorm(1, dim))\n",
    "\n",
    "  # --------------------------------------------------------------------------------------------\n",
    "\n",
    "  # Layer Application Function\n",
    "  def forward(\n",
    "      self,\n",
    "      x\n",
    "  ):\n",
    "\n",
    "    # Layer Application\n",
    "    b, c, h, w = x.shape; qkv = self.conv1(x).chunk(3, dim = 1)\n",
    "    q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)',\n",
    "                                      h = self.num_head), qkv)\n",
    "\n",
    "    # Quadratic Attention Layer Application\n",
    "    if self.layer == 'quadratic':\n",
    "      q = q * self.scale\n",
    "      sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "      sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "      attn = sim.softmax(dim = -1)\n",
    "      out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "      out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "\n",
    "    # Linear Attention Layer Application\n",
    "    elif self.layer == 'linear':\n",
    "      q = q.softmax(dim = -2) * self.scale; k = k.softmax(dim = -1)\n",
    "      context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "      out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "      out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.num_head, x = h, y = w)\n",
    "\n",
    "    else: print(f\"ERROR: '{self.layer}' is not a valid Attention Layer type!\"); return 0\n",
    "    return self.conv2(out)\n",
    "\n",
    "# ============================================================================================\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
